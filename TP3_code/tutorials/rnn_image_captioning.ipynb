{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWeaQPS2bNgQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Partie 2: RNNs et description d'images\n",
    "\n",
    "Fait par **Antoine Théberge** de l'Université de Sherbrooke, Canada.\n",
    "\n",
    "Inspiré par la partie sur les auto-encodeurs et [un notebook similaire.](https://colab.research.google.com/github/jaygala24/pytorch-implementations/blob/master/Show%2C%20Attend%20and%20Tell.ipynb).\n",
    "\n",
    "Dans ce notebook, nous verrons comment construire un réseau récurrent avec `pytorch` et ce, dans le but de faire de la description d'images. Le réseau aura deux parties: un encodeur convolutionnel pour extraire des caractéristiques (*features*) de l'image.  Ces caractéristiques seront ensuite  données au décodeur, un LSTM, afin de produire des jetons de mots (*word tokens*) décrivant l'image.\n",
    "\n",
    "Si vous exécutez ce notebook, il est *fortement* recommendé d'utiliser Google Colab. Si vous souhaitez tout de même entraîner le modèle sur votre ordinateur et utiliser les jeux de données, assurez vous d'avoir environs 12Gb de VRAM et 40GBb d'espace disque. \n",
    "\n",
    "Premièrement, un peu de poutine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3813,
     "status": "ok",
     "timestamp": 1674768806975,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "_udiVZ_AMt9R",
    "outputId": "a7b97136-becb-4e72-c4b6-00756f47111a"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True) # Change according to your own Google Drive config\n",
    "# !cp -r drive/MyDrive/IFT780/tp3/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1674768809356,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "HaWV_VutbNgS",
    "outputId": "c5728eff-f80e-4c45-88fc-3496fac8828d",
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zwzk7S67bNgS",
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture packages_install\n",
    "\n",
    "# # Make sure the repo's package and its dependencies are installed\n",
    "# !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ErGBkApdbNgT",
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture project_path_setup\n",
    "\n",
    "import sys\n",
    "\n",
    "if '../' in sys.path:\n",
    "    print(sys.path)\n",
    "else:\n",
    "    sys.path.append('../')\n",
    "    print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiQnoFZEbNgT"
   },
   "source": [
    "Téléchargeons les données. Ceci devrait prendre de 10 à 20 minutes sur Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2512509,
     "status": "ok",
     "timestamp": 1674771348652,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "Oj0cBFHFbNgT",
    "outputId": "20d1ec40-ad9e-4632-c3d5-f40c1db8e8ae"
   },
   "outputs": [],
   "source": [
    "!wget -nc http://images.cocodataset.org/zips/train2017.zip\n",
    "!unzip -q 'train2017.zip'\n",
    "!rm 'train2017.zip'\n",
    "\n",
    "!wget -nc http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip -q 'val2017.zip'\n",
    "!rm 'val2017.zip'\n",
    "\n",
    "!wget -nc http://images.cocodataset.org/zips/test2017.zip\n",
    "!unzip -q 'test2017.zip'\n",
    "!rm 'test2017.zip'\n",
    "\n",
    "!wget -nc http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip -q 'annotations_trainval2017.zip'\n",
    "\n",
    "!wget -nc http://images.cocodataset.org/annotations/image_info_test2017.zip\n",
    "!unzip -q 'image_info_test2017.zip'\n",
    "\n",
    "!wget -nc http://cs.stanford.edu/people/karpathy/deepimagesent/caption_datasets.zip\n",
    "!unzip -q 'caption_datasets.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSccq-uebNgU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Regardons nos données\n",
    "\n",
    "Dans ce notebook, nous travaillerons sur le jeu de données [COCO](https://cocodataset.org/). COCO contient quelques centaines de millier d'images avec pour chacune une description textuelle (*image captionning*), mais aussi la détection d'objets et la segmentation.\n",
    "\n",
    "Concentrons nous sur la description d'images, regardons quelques images du jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gynlc4ZcbNgU"
   },
   "outputs": [],
   "source": [
    "def display_images_with_captions(\n",
    "    images, captions, decode_ids=False, vocab=None, token_formatter=None\n",
    "):\n",
    "    num_samples = len(images)\n",
    "    plt.axis('off')\n",
    "\n",
    "    fig, axes = plt.subplots(figsize=(10, len(images) * 0.5 * 2), nrows=num_samples, ncols=2)\n",
    "\n",
    "    def _display_sample(ax_idx, sample, sample_label) -> None:\n",
    "        ax = plt.subplot(num_samples, 2, ax_idx)\n",
    "        ax.imshow(sample.detach().cpu().numpy().transpose((1, 2, 0)))\n",
    "        ax.axis('off')\n",
    "\n",
    "        ax = plt.subplot(num_samples, 2, ax_idx + 1)\n",
    "        if decode_ids and vocab:\n",
    "            text = [vocab.lookup_token(i) for i in sample_label]\n",
    "            ax.text(0.25, 0.5, token_formatter(text), wrap=True)\n",
    "        else:\n",
    "            ax.text(0.25, 0.5, sample_label[0], wrap=True)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Display the sample for the current column from each data source\n",
    "    for src_idx, (img, cap) in enumerate(zip(images, captions)):\n",
    "        _display_sample((src_idx * 2) + 1, img, cap)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8821,
     "status": "ok",
     "timestamp": 1674771357460,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "E1vzmsfybNgU",
    "outputId": "3492fa77-1623-4aac-c4e7-d5c0447ccacd",
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# COCO has variable image sizes, so we will resize them to 256x256.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the data\n",
    "coco2017 = CocoCaptions(root=\"./train2017\", annFile=\"./annotations/captions_train2017.json\", transform=transform)\n",
    "\n",
    "# Check data by displaying random images\n",
    "samples_indices = np.random.randint(len(coco2017), size=10)\n",
    "coco_img_list = [coco2017[sample_idx][0] for sample_idx in samples_indices]\n",
    "coco_cap_list = [coco2017[sample_idx][1] for sample_idx in samples_indices]\n",
    "display_images_with_captions(coco_img_list, coco_cap_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcDZlV1hbNgV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Le jeu de données COCO contient plusieurs descriptions par images, mais nous n'en affichons qu'une seule. Vous pouvez ré-exécuter la cellule afin de visualier différentes paires d'images/descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvKt_cKabNgV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "En regardant directement les données, on peut voir la forme que prennent celles-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1674771357461,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "8TWVUyaqbNgV",
    "outputId": "16daf212-fe21-4c38-c313-41c93a43bc1c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the first training image and its class label\n",
    "sample_image = coco2017[0][0] # sample_image is a \"PyTorch tensor\"\n",
    "sample_labels = coco2017[0][1] \n",
    "\n",
    "# Convert the Tensor into a numpy array\n",
    "sample_image_np = sample_image.numpy()  \n",
    "print(\"Image size = \", sample_image_np.shape)\n",
    "\n",
    "# Call \"transpose\" to go from C*W*H to H*W*C\n",
    "sample_image_np = sample_image_np.transpose((1,2,0))\n",
    "print(\"Image size = \", sample_image_np.shape)\n",
    "\n",
    "# Plot\n",
    "plt.imshow(sample_image_np)\n",
    "print(\"The image label is \", sample_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko7RkxT6bNgW"
   },
   "source": [
    "### Transformations et jetonisations du text\n",
    "\n",
    "Ici, nous appliquerons quelques transformations au jeu de données afin de le rendre utilisable par notre modèle. Premièrement, nous allons \"jetoniser\" (*tokenize*) le text.\n",
    "\n",
    "Plusieurs options s'offrent à nous pour la granularité des jetons. Voulons-nous un jeton par caractère ? Un jeton par mot ? Quelque chose entre les deux ? Pour ce notebook, nous prendrons un ensemble de jetons déjà précalculés pour nous, correspondant à chaque mot dans l'ensemble des descriptions. Premièrement, définissons quelques classes qui nous serons utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuiyxH9ObNgW"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "# Transform to select only the first n captions out of all                                                                                                                         \n",
    "class NCaptionTransform(Module):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def __call__(self, target):\n",
    "        if self.n == 1:\n",
    "          return target[0]\n",
    "        else:\n",
    "          return target[:self.n]\n",
    "    \n",
    "# Define a tokenizer transform to automatically transform string captions into tokens\n",
    "# (not sure why this is not built in torchtext)\n",
    "class TokenizerTransform(Module):\n",
    "    def __init__(self, tokenizer):\n",
    "        super(TokenizerTransform).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __call__(self, captions):\n",
    "        return [self.tokenizer(c) for c in captions]\n",
    "\n",
    "# Last, we will need a way to obtain sentences from a list of tokens when we will evaluate the model ourselves\n",
    "class TokenFormatter(object):                                                                                                                                                                                \n",
    "    def __init__(self, start_token, end_token, pad_token):\n",
    "        self.bos = start_token\n",
    "        self.eos = end_token\n",
    "        self.pad = pad_token\n",
    "        self.filter = [self.bos, self.eos, self.pad]\n",
    "    def __call__(self, tokens):  \n",
    "        tokens = list(filter(lambda t: t not in self.filter, tokens))\n",
    "        return ' '.join(tokens)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgykhfA6bNgW"
   },
   "source": [
    "Ensuite, définissons des jetons spéciaux et batissons notre vocabulaire, soit l'ensemble de tous les jetons que notre modèle pourra prédire.\n",
    "\n",
    "## Question:\n",
    "* À quoi servent les jetons spéciaux \\[START\\], \\[END\\], \\[PAD\\], \\[UNK\\] ? Donnez une réponse par jeton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3pKR41abNgW"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torchtext.transforms as T\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import vocab, build_vocab_from_iterator    \n",
    "\n",
    "# Captions will be truncated or padded to 20 tokens.\n",
    "max_seq_len = 20\n",
    "\n",
    "# Images will be resized to 256x256\n",
    "data_shape = (256, 256)\n",
    "    \n",
    "# We define special tokens\n",
    "start_token = \"[START]\"\n",
    "end_token = \"[END]\"\n",
    "pad_token = \"[PAD]\"\n",
    "unk_token = \"[UNK]\"\n",
    "\n",
    "with open('./dataset_coco.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    # We build a vocabulary from karpathy's tokens + punctuation and whitespace.\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        (s[\"tokens\"] for i in data[\"images\"] for s in i[\"sentences\"]),\n",
    "        specials=[start_token, end_token, pad_token, unk_token] + list(string.whitespace)+ list(string.punctuation), \n",
    "        special_first=True)\n",
    "\n",
    "unknown_token = vocab([\"[UNK]\"])[0]\n",
    "vocab.set_default_index(unknown_token)\n",
    "\n",
    "# Keep the ids of special tokens in the vocabulary\n",
    "# These will be useful later.\n",
    "BOS_IDX, EOS_IDX, PAD_IDX, UNK_IDK = vocab.lookup_indices([start_token, end_token, pad_token, unk_token])\n",
    "\n",
    "eng_tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Define a series of transformations to apply on the targets (the captions)\n",
    "text_transform = T.Sequential(\n",
    "    TokenizerTransform(eng_tokenizer), # transform the textual captions into a series of tokens.\n",
    "    T.VocabTransform(vocab), # transform the tokens into ids.\n",
    "    T.AddToken(token=BOS_IDX, begin=True), # automatically prepend the the start token.\n",
    "    T.AddToken(token=EOS_IDX, begin=False), # automatically append the the end token.\n",
    "    T.Truncate(max_seq_len), # truncate the sequence to the maximum length. .\n",
    "    NCaptionTransform(1), # only extract one caption amongst all available .\n",
    "    T.ToTensor(PAD_IDX, dtype=torch.int64), # transform the targets into a tensor.\n",
    "    T.PadTransform(max_seq_len, PAD_IDX)) # pad the captions, if they weren't truncated, to the maximum length.\n",
    "\n",
    "# Transformations on the images.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Cast them as tensors \n",
    "    transforms.Resize(data_shape), # Resize the tensors to 256x256\n",
    "])\n",
    "\n",
    "# Instanciate the token formatter. This will allow us to write actual decoded captions from tokens.\n",
    "token_formatter = TokenFormatter(start_token, end_token, pad_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxmQOIIHbNgW"
   },
   "source": [
    "Maintenant que nous avons géré la jetonisation et les transformations, nous pouvons instancier nos jeux de données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1674771363862,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "9JAAZ9S6bNgW",
    "outputId": "a491ccae-a3ec-4d51-e74d-47a6751ec90c"
   },
   "outputs": [],
   "source": [
    "# Train, val and test sets are already split for us. Note that the test set has no transformations applied to the \n",
    "# captions, as ground truth captions are not available on the test set.\n",
    "coco2017_train = CocoCaptions(root=\"./train2017\", annFile=\"./annotations/captions_train2017.json\", transform=transform, target_transform=text_transform)\n",
    "coco2017_val = CocoCaptions(root=\"./val2017\", annFile=\"./annotations/captions_val2017.json\", transform=transform, target_transform=text_transform)\n",
    "coco2017_test = CocoCaptions(root=\"./test2017\", annFile=\"./annotations/image_info_test2017.json\", transform=transform)\n",
    "\n",
    "# Get the first training image and its class label\n",
    "sample_image = coco2017_val[0][0] # sample_image is a \"PyTorch tensor\"\n",
    "sample_labels = coco2017_val[0][1] # sample label is a list of captions\n",
    "\n",
    "print(\"Tokenized and ided caption is\", sample_labels)\n",
    "\n",
    "display_images_with_captions(sample_image[None, ...], sample_labels[None, ...], decode_ids=True, vocab=vocab, token_formatter=token_formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9Tjj8_IbNgX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Développons notre RNN !  \n",
    "\n",
    "Nous sommes maintenant rendu à l'étape de la conception du RNN. Premièrement, définissons notre encodeur, un Resnet50 pré-entraîné. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HH0lNjVObNgX"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    \"\"\" Encoder. Uses a pretrained resetnet50 model to project images to features.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        \n",
    "        # Base ResNet\n",
    "        # The original impl. uses a VGGNet which outputs 14x14x512 feature maps\n",
    "        # We will add a pooling layer to output 14x14x2048 instead of 8x8x2048\n",
    "        base = resnet50(pretrained=True) \n",
    "        for param in base.parameters():\n",
    "            param.requires_grad = False\n",
    "        # remove last layers (avg pool and fc) of ResNet usually used for classification\n",
    "        pooling = nn.AdaptiveAvgPool2d((14, 14))\n",
    "        modules = list(base.children())[:-2] + [pooling]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        \n",
    "        Args:\n",
    "        images: images, a tensor of size (batch_size, 3, 256, 256)\n",
    "        \n",
    "        Returns:\n",
    "        out: features, tensor of size (batch_size, 2048, 14, 14)\n",
    "        \"\"\"\n",
    "        out = self.model(images) # (batch_size, 2048, 14, 14)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OinrZ7WkFLwZ"
   },
   "source": [
    "# Question:\n",
    "\n",
    "* Pourquoi utiliser un modèle pré-entraîné ? Pourquoi ne pas entraîner aussi l'encodeur ?\n",
    "\n",
    "* Expliquez dans vos propres mots quelle est la représentation de l'image en sortie de l'encodeur. Considérez la taille du tenseur, ce qu'il représente, et l'architecture Resnet50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-HM050zFb5o"
   },
   "source": [
    "Maintenant le décodeur. Celui-ci est un LSTM qui recevra en entrée le dernier jeton prédit, l'image en entrée et ses états cachés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqqqt3cebNgX",
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "    \n",
    "class RNNDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size, decoder_size, vocab_size, device):\n",
    "        \"\"\" Decoder. Uses an RNN (LSTM) to predict captions. Uses its hidden features (h, c), the last\n",
    "        (embedded) token and features of the input image to predict the next token.\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.encoder_size = 2048 # set by ResNet. Could do a 1x1 conv to change it\n",
    "        self.embed_size = embed_size\n",
    "        self.decoder_size = decoder_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_IDX) # embedding layer        \n",
    "        self.features_pool = nn.AdaptiveAvgPool2d((1, 1)) # To go from 14x14x2048 to 1x1x2048\n",
    "        \n",
    "        self.init_h = nn.Linear(self.encoder_size, decoder_size)   # To initialize h. Could also be set to 0 \n",
    "        self.init_c = nn.Linear(self.encoder_size, decoder_size)    # To initialize c. Could also be set to 0.\n",
    "        self.rnn = nn.LSTMCell(self.encoder_size + embed_size, decoder_size, bias=True) # Actual decoder\n",
    "        self.fc = nn.Linear(decoder_size, vocab_size)        # To map from RNN features to class scores\n",
    "        self.dropout = nn.Dropout(p=0.5) # To avoid overfitting\n",
    "    \n",
    "    def init_hidden_state(self, features):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        \n",
    "        \"\"\"\n",
    "        h = self.init_h(features)\n",
    "        c = self.init_c(features)\n",
    "        return h, c\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Decoding forward prop.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # Batch size, max_seq_len\n",
    "        B, T  = captions.size()\n",
    "\n",
    "        # Average out feature \"pixels\" into one\n",
    "        # Could also use a single conv. or flatten the features then feed to a lin. layer\n",
    "        features = self.features_pool(features)\n",
    "        # Batch size, 2048, 1, 1\n",
    "        B, C, H, W = features.size()\n",
    "        # Batch size, self.encoder_size\n",
    "        features = features.reshape(B, H * W, C).squeeze(1)\n",
    "\n",
    "        # Word embedding based on ground-truth captions\n",
    "        embeddings = self.word_embedding(captions)\n",
    "        \n",
    "        # Placeholder tensor for scores of each class at each timestep\n",
    "        # Size of batch size, caption max length, vocab size\n",
    "        scores = torch.zeros((B, max_seq_len, self.vocab_size)).to(self.device)\n",
    "        \n",
    "        # Init h,c based on encoded features\n",
    "        h, c = self.init_hidden_state(features)\n",
    "        \n",
    "        # Time to actually decode\n",
    "        for t in range(T):\n",
    "\n",
    "            # Decode based on the current (embedded) token, the image features and the hidden states \n",
    "            h, c = self.rnn(\n",
    "                torch.cat([embeddings[:, t, :], features], dim=-1), (h, c))\n",
    "\n",
    "            # Predict based on new hidden features\n",
    "            scores_t = self.fc(self.dropout(h))\n",
    "\n",
    "            # Save the predictions at time t\n",
    "            scores[:, t, :] = scores_t\n",
    "    \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, features, T):\n",
    "        \"\"\"\n",
    "        Inference propagation.\n",
    "        \n",
    "        Args:\n",
    "        features: tensor of sizeension (batch_size, enc_image_size, enc_image_size, encoder_size)\n",
    "        max_seq_len: maximum caption length.\n",
    "        \n",
    "        Returns:\n",
    "        captions: padded predicted captions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Explicitely don't compute gradients\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Average out feature \"pixels\" into one\n",
    "            # Could also use a single conv. or flatten the features then feed to a lin. layer\n",
    "            features = self.features_pool(features)\n",
    "            # Batch size, 2048, 1, 1\n",
    "            B, C, H, W = features.size()\n",
    "            # Batch size, self.encoder_size\n",
    "            features = features.reshape(B, H * W, C).squeeze(1)\n",
    "            \n",
    "            # Placeholder tensor for predicted token idx\n",
    "            # Size of batch size, caption max length, vocab size\n",
    "            # pad_idx is passed \"globally\" which is kind of dirty.\n",
    "            captions = torch.full((B, T), PAD_IDX, dtype=torch.int64).to(self.device)\n",
    "            # Set [START] at T=0\n",
    "            captions[:, 0] = BOS_IDX\n",
    "        \n",
    "            # Init h,c based on encoded features\n",
    "            h, c = self.init_hidden_state(features)\n",
    "\n",
    "            # Keep track of which caption is complete.\n",
    "            stopped = np.asarray([False] * B)\n",
    "            \n",
    "            for t in range(T-1):\n",
    "                # Embed at each timestep since embedding changes\n",
    "                embeddings = self.word_embedding(captions)\n",
    "\n",
    "                # Decode based on the current (embedded) token, the image features and the hidden states \n",
    "                h, c = self.rnn(\n",
    "                    torch.cat([embeddings[:, t, :], features], dim=-1), (h, c))\n",
    "\n",
    "                # get the next word prediction\n",
    "                scores = self.fc(self.dropout(h))                \n",
    "                pred_t = torch.argmax(scores, dim=-1)\n",
    "                                \n",
    "                # Only keep captions which are not over\n",
    "                captions[~stopped, t+1] = pred_t[~stopped]\n",
    "\n",
    "                # See if the caption is complete i.e. [END] token has been predicted\n",
    "                stopped = np.logical_or(stopped, pred_t.cpu().numpy() == EOS_IDX)\n",
    "                \n",
    "                # No point in predicting if everything has stopped.\n",
    "                if np.all(stopped):\n",
    "                    break\n",
    "\n",
    "            return captions.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAo5V2ghbNgX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Questions:\n",
    "\n",
    "* Quel est l'utilité de la \"embedding layer\" ?\n",
    "* Que représente la sortie du modèle dans la fonction `forward` ? Et dans la fonction `predict` ?\n",
    "* Quelles sont les différences entre l'entrée du LSTM dans la fonction `forward` et la fonction `predict` ? Quels sont les avantages d'avoir ces différences ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "za5fxv2-bNgX"
   },
   "source": [
    "Définissons la propagation avant, qui calculera aussi la fonction de perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSwrOawtbNgY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "def collate_fn(data):\n",
    "    # Not sure why there is no built-in way of doing this in torch\n",
    "    images, captions = map(torch.stack, zip(*data))\n",
    "    return images, captions\n",
    "\n",
    "def get_lengths_of_padded_captions(captions, pad_idx):\n",
    "    lengths = np.zeros(captions.shape[0])\n",
    "    for i, c in enumerate(captions):\n",
    "        for j, char in enumerate(c):\n",
    "            if char == pad_idx:\n",
    "                break\n",
    "            lengths[i] = j\n",
    "    return lengths\n",
    "\n",
    "def forward_pass(encoder, decoder, loss_fn, images, captions, lengths):\n",
    "    \"\"\"Encoder-Decoder forward pass.\n",
    "\n",
    "    Args:\n",
    "        encoder_decoder: neural net that predicts a caption given an image\n",
    "        images: batch of N COCO images\n",
    "        captions: batch of N target captions for the images\n",
    "\n",
    "    Returns:\n",
    "        loss: crossentropy loss\n",
    "        predictions: batch of N predicted captions\n",
    "    \"\"\"\n",
    "\n",
    "    # Project the image to features\n",
    "    features = encoder(images)\n",
    "    # Feed them to the decoder to obtain captions\n",
    "    predictions = decoder(features, captions)\n",
    "    # We did not decode the [START] token\n",
    "    targets = captions[:, 1:]        \n",
    "    # \"Pack\" predictions and targets by removing all padding tokens and reshaping the result into a \"flat\" tensor\n",
    "    packed_predictions, packed_captions = (pack_padded_sequence(predictions, lengths, batch_first=True, enforce_sorted=False), \n",
    "                                           pack_padded_sequence(targets, lengths, batch_first=True, enforce_sorted=False))\n",
    "    loss = loss_fn(packed_predictions.data, packed_captions.data).mean()  # Compute the reconstruction loss\n",
    "    \n",
    "    return loss, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIiKg7EabNgY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Boucles d'entraînement\n",
    "\n",
    "Avant d'entraîner notre modèle, nous devons définir des boucles d'entraînement. Premièrement, une boucle de sur-apprentissage permettant de valider que notre modèle fonctionne et peut sur-apprendre sur un petit ensemble de données. Puis, la boucle d'entraînement. Celle-ci itérera sur toutes les données en entraînement afin de calculer et rétro-propager la fonction de perte, puis calculera la fonction de perte sur les données en validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QQBNMFIbNgY",
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def overfit(encoder, decoder, forward_pass, optimizer, loss_fn, train_dataloader, device):\n",
    "    \"\"\" Overfitting method to ensure everything is properly implemented.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper to visualize the training progression\n",
    "    fit_pbar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    pbar_metrics = {\"train_loss\": None}\n",
    "    \n",
    "    images, captions = next(iter(train_dataloader)) # Only sample the first batch of the dataloader\n",
    "    train_losses = []\n",
    "    # Train for n epochs on the same data\n",
    "    for epoch in fit_pbar:\n",
    "        optimizer.zero_grad()   # Make sure gradients are reset\n",
    "        # Get the length of each caption without padding to compute the loss\n",
    "        # We don't want to penalize the model for not predicting the padding token\n",
    "        lengths = get_lengths_of_padded_captions(captions, PAD_IDX)\n",
    "        train_loss, _ = forward_pass(encoder, decoder, loss_fn, images.to(device), captions.to(device), lengths)    # Forward pass\n",
    "        train_loss.backward()   # Backward pass\n",
    "        optimizer.step()    # Update parameters w.r.t. optimizer and gradients\n",
    "        train_losses.append(train_loss.item())\n",
    "        pbar_metrics[\"train_loss\"] = np.mean(train_losses)\n",
    "        fit_pbar.set_postfix(pbar_metrics)\n",
    "\n",
    "        \n",
    "def train(encoder, decoder, forward_pass, optimizer, loss_fn, train_dataloader, val_dataloader, device):\n",
    "    \"\"\" Training loop.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Helper to visualize the training progression\n",
    "    fit_pbar = tqdm(range(epochs), desc=\"Training\", unit=\"epoch\")\n",
    "    pbar_metrics = {\"train_loss\": None, \"val_loss\": None}\n",
    "    \n",
    "    # For each epoch\n",
    "    for i, epoch in enumerate(fit_pbar):\n",
    "        \n",
    "        torch.save(decoder.state_dict(), '{}_{}.pth'.format(decoder.__class__.__name__, i))\n",
    "        \n",
    "        decoder.train()\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        # Train over all the training data\n",
    "        for images, captions in train_dataloader:\n",
    "            optimizer.zero_grad()   # Make sure gradients are reset\n",
    "            # Get the length of each caption without padding to compute the loss\n",
    "            # We don't want to penalize the model for not predicting the padding token\n",
    "            lengths = get_lengths_of_padded_captions(captions, PAD_IDX)\n",
    "            # Compute the forward pass and the corresponding loss\n",
    "            train_loss, _ = forward_pass(\n",
    "                encoder, decoder, loss_fn, \n",
    "                images.to(device), captions.to(device), lengths)    # Forward pass\n",
    "            train_loss.backward()   # Backpropagate the loss to compute gradients\n",
    "            optimizer.step()    # Update parameters w.r.t. optimizer and gradients\n",
    "            \n",
    "            train_losses.append(train_loss.item())\n",
    "            pbar_metrics[\"train_loss\"] = np.mean(train_losses)\n",
    "            fit_pbar.set_postfix(pbar_metrics)\n",
    "        \n",
    "        decoder.eval()\n",
    "        # At the end of the epoch, check performance against the validation data\n",
    "        for images, captions in val_dataloader:\n",
    "            # Don't compute gradients during validation\n",
    "            with torch.no_grad():\n",
    "                # Get the length of each caption without padding to compute the loss\n",
    "                # We don't want to penalize the model for not predicting the padding token\n",
    "                lengths = get_lengths_of_padded_captions(captions, PAD_IDX)\n",
    "                # Compute the forward pass and the corresponding loss\n",
    "                val_loss, _ = forward_pass(\n",
    "                    encoder, decoder, loss_fn, \n",
    "                    images.to(device), captions.to(device), lengths)            \n",
    "                val_losses.append(val_loss.item())\n",
    "                pbar_metrics[\"val_loss\"] = np.mean(val_losses)\n",
    "                \n",
    "        print(pbar_metrics)\n",
    "        \n",
    "        torch.save(decoder.state_dict(), '{}_{}.pth'.format(decoder.__class__.__name__, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAIMlLYabNgY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Avant d'entraîner le modèle, essayons de sur-apprendre sur un petit ensemble de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240,
     "referenced_widgets": [
      "eee0e3462f2e4ab7bd67138c76f70835",
      "556e6e8219b24525a2d617f681a7eb4e",
      "748a88c0d3dc4dc1804abbe8fb965f09",
      "1a8a4eb4d1bd40dfaa75e7132045bb08",
      "f9873dff5d684e558ee8cb3e91fd6343",
      "e7c42854744a41f3befb8d94e11e8569",
      "03f08900a93f49978ed1c0eec1935b73",
      "d38f66105780496096a1938171b7684f",
      "ffc86dccd84b495d99c2afe1ba29f2e6",
      "b6ca0eb5b52a44d2ab1c5a3ef9b44e21",
      "985a064893f54dde8768961c94385bab",
      "c2b76932f6bc47d88849f9cd94d0be41",
      "13c356d920864bd69ac525d804c88616",
      "d591ed72b8f6485a837a281a02c46548",
      "033f1884b5824b1c94a16ca824448ed7",
      "0e5f8561330f4818bf48cbb302524b04",
      "35b7a4545f60452bb0dd06e291a938ff",
      "7866c5c0bc594ddfb36bcd26b27abce1",
      "316439ab52b34990b53451b8b5e750e7",
      "ca9add9b81664a96a55f4cf3fd03c053",
      "8948573319f8422d9d224f6d682ea882",
      "f1c8cd1e18e74610a943b2e05ab96781"
     ]
    },
    "executionInfo": {
     "elapsed": 25799,
     "status": "ok",
     "timestamp": 1674771390203,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "crjFiqmObNgY",
    "outputId": "05221912-9c4d-462f-943e-18d9d15b0f2a",
    "pycharm": {
     "name": "#%% code\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define some overfitting hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 10 # Only train on 10 images\n",
    "embedding_size = 512\n",
    "decoder_size = 512\n",
    "vocab_size = len(vocab)\n",
    "lr = 1e-3\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "# Instanciate the encoder\n",
    "encoder = CNNEncoder()\n",
    "# Instanciate the decoder\n",
    "decoder = RNNDecoder(embedding_size, decoder_size, vocab_size, device)\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "# Define the optimizer. Note that only the decoder's parameters will be optimized\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "# Send the models to the GPU (if available)\n",
    "encoder, decoder = encoder.to(device), decoder.to(device)\n",
    "\n",
    "# Define the dataloader. Only its first batch will be used.\n",
    "overfit_dataloader = DataLoader(\n",
    "    coco2017_train, batch_size=batch_size, shuffle=False, num_workers=8, \n",
    "    collate_fn=collate_fn, pin_memory=device==\"cuda\")\n",
    "\n",
    "# Start the overfitting process\n",
    "overfit(encoder, decoder, forward_pass, optimizer, loss_fn, overfit_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGBJen7NJI2_"
   },
   "source": [
    "## Question\n",
    "\n",
    "* Pourquoi utilser l'entropie-croisée comme fonction de perte ? Pourquoi pas une fonction de perte `l2`, par exemple ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAkQvTuBbNgY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Voyons voir le résultat sur les mêmes données. Définissons nous d'abord une fonction permettant de prédire simplement, sans calculer la fonction de perte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NKZCLNVbNgY"
   },
   "outputs": [],
   "source": [
    "def predict_pass(encoder, decoder, images, max_seq_len):\n",
    "    \"\"\"Encoder-Decoder prediction pass.\n",
    "\n",
    "    Args:\n",
    "        encoder: neural net that output features given an image\n",
    "        decoder: \n",
    "        images: batch of N COCO images\n",
    "        captions: batch of N target captions for the images\n",
    "\n",
    "    Returns:\n",
    "        predictions: predicted captions\n",
    "        lengths: batch of N integers representing the length of each captions\n",
    "    \"\"\"\n",
    "    # Project the image to features\n",
    "    features = encoder(images.to(device))\n",
    "    # Feed them to the decoder to obtain captions\n",
    "    predictions = decoder.predict(features, max_seq_len)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiVTeVjxbNgY"
   },
   "source": [
    "Puis, visualisons les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2133,
     "status": "ok",
     "timestamp": 1674771495946,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "AIttHBPXbNgY",
    "outputId": "6f3e29cb-6773-428c-8d24-aae8e774b5fc",
    "pycharm": {
     "name": "#%% code\n"
    }
   },
   "outputs": [],
   "source": [
    "# Declare a new dataloader to visualize the overfitting results\n",
    "overfit_test_dataloader = DataLoader(\n",
    "    coco2017_train, batch_size=5, shuffle=False, num_workers=1, \n",
    "    collate_fn=collate_fn, pin_memory=device==\"cuda\")\n",
    "\n",
    "# Sample the first batch, same as in the overfitting procedure\n",
    "images, captions = next(iter(overfit_test_dataloader))\n",
    "\n",
    "# Predict captions for the images\n",
    "predictions = predict_pass(encoder, decoder, images, max_seq_len)\n",
    "\n",
    "print(\"Predicted captions\")\n",
    "display_images_with_captions(images, predictions, decode_ids=True, vocab=vocab, token_formatter=token_formatter)\n",
    "\n",
    "print(\"Ground truth captions\")\n",
    "display_images_with_captions(images, captions, decode_ids=True, vocab=vocab, token_formatter=token_formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3sai2sMbNgZ"
   },
   "source": [
    "Les résultats devraient être convainquants. C'est maintenant le temps d'entraîner ! De par la taille du jeu de données, seulement quelques itérations sur le jeu d'entraînement devraient suffir. Ça prend environs 90 minutes entraîner le modèle pendant 10 epochs sur ma carte graphique 1080ti. Les temps sont variables sur Google Collab et peuvent être supérieurs. Si vous souhaitez entraîner sur votre propre ordinateur, vous pouvez réduire ou augmenter la taille de la *batch*, mais vous ne devriez pas avoir besoin de changer autre chose.\n",
    "\n",
    "**Si vous ne souhaitez pas entraîner**, vous pouvez charger directement les paramètres du modèle à la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pb7Vx6JrbNgZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Define some training hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "embedding_size = 512\n",
    "decoder_size = 512\n",
    "vocab_size = len(vocab)\n",
    "lr = 4e-4\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Instanciate the encoder\n",
    "encoder = CNNEncoder()\n",
    "# Instanciate the decoder\n",
    "decoder = RNNDecoder(embedding_size, decoder_size, vocab_size, device)\n",
    "# Define the loss function\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "# Define the optimizer. Note that only the decoder's parameters will be optimized\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "# Send the models to the GPU (if available)\n",
    "encoder, decoder = encoder.to(device), decoder.to(device)\n",
    "\n",
    "# Training and validation dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    coco2017_train, batch_size=batch_size, shuffle=False, num_workers=10, \n",
    "    collate_fn=collate_fn, pin_memory=device==\"cuda\")\n",
    "val_dataloader = DataLoader(\n",
    "    coco2017_val, batch_size=batch_size, shuffle=False, num_workers=10, \n",
    "    collate_fn=collate_fn, pin_memory=device==\"cuda\")\n",
    "\n",
    "train(encoder, decoder, forward_pass, optimizer, loss_fn, train_dataloader, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByLiuQj53PSE"
   },
   "source": [
    "Si vous avez entraîné le modèle, vous pouvez ignorer la cellule suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWlCPH6OcMLC"
   },
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "decoder_size = 512\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Instanciate the encoder\n",
    "encoder = CNNEncoder()\n",
    "# Instanciate the decoder\n",
    "decoder = RNNDecoder(embedding_size, decoder_size, vocab_size, device)\n",
    "decoder.load_state_dict(torch.load('weights/RNNDecoder.pth'))\n",
    "# Send the models to the GPU (if available)\n",
    "encoder, decoder = encoder.to(device), decoder.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNaJFpbnbNgZ"
   },
   "source": [
    "Une fois l'entraînement terminé, visualisons les résultats sur le jeu d'entraînement. Puisque nous n'avons pas les descriptions des images dans le jeu de test, vous devrez juger de la qualité de la description par vous-même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "executionInfo": {
     "elapsed": 2143,
     "status": "ok",
     "timestamp": 1674771553616,
     "user": {
      "displayName": "Pierre-Marc Jodoin",
      "userId": "12933469182941615712"
     },
     "user_tz": 300
    },
    "id": "UEYZAoFPbNgZ",
    "outputId": "27c5a6d6-ece2-4a70-a1a4-ec9fccf3a775"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    coco2017_test, batch_size=5, shuffle=True, num_workers=1)\n",
    "\n",
    "images, _ = next(iter(test_dataloader))\n",
    "\n",
    "predictions = predict_pass(encoder, decoder, images, max_seq_len)\n",
    "\n",
    "display_images_with_captions(images, predictions, decode_ids=True, vocab=vocab, token_formatter=token_formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Veev1hxYbNgZ"
   },
   "source": [
    "Les résultats devraient être généralement bien, mais certaines erreurs devraient s'introduire dans les résultats. Vous pouvez exécuter la cellule précédente plusieurs fois pour obtenir des résultats différents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loHp2gMDbNgZ"
   },
   "source": [
    "## Questions:\n",
    "\n",
    "* Les descriptions générées à la cellule précédente devraient parfois manquer de précision, inclure des éléments de trop ou en omettre (\"*a group of cows standing ...*\" versus \"*a cow standing ...*\", par exemple). Pourquoi croyez-vous que cela se produit ? Prenez en compte l'architecture des deux modèles et les tenseurs échangés entre ceux-ci. Indice: ce n'est pas dû à l'entraînement ou parce que le modèle n'a pas convergé ou par manque de données. Si vous êtes courageux.euses, vous pouvez entraîner le modèle plus longtemps pour vous en convaincre.\n",
    "* Pour cette implémentation, nous avons utilisé un ensemble de jetons correspondant grosso modo à chaque mot dans l'ensemble de toutes les descriptions. Combien de jetons font partie du vocabulaire du modèle ?\n",
    "* Quel aurait été l'impact sur le modèle, sur son nombre de paramètres et sur la difficulté du problème si nous avions choisi d'avoir plutôt un jeton par caractère ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkymAAO3N8ce"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033f1884b5824b1c94a16ca824448ed7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8948573319f8422d9d224f6d682ea882",
      "placeholder": "​",
      "style": "IPY_MODEL_f1c8cd1e18e74610a943b2e05ab96781",
      "value": " 100/100 [00:19&lt;00:00,  9.00epoch/s, train_loss=1.3]"
     }
    },
    "03f08900a93f49978ed1c0eec1935b73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e5f8561330f4818bf48cbb302524b04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13c356d920864bd69ac525d804c88616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35b7a4545f60452bb0dd06e291a938ff",
      "placeholder": "​",
      "style": "IPY_MODEL_7866c5c0bc594ddfb36bcd26b27abce1",
      "value": "Training: 100%"
     }
    },
    "1a8a4eb4d1bd40dfaa75e7132045bb08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6ca0eb5b52a44d2ab1c5a3ef9b44e21",
      "placeholder": "​",
      "style": "IPY_MODEL_985a064893f54dde8768961c94385bab",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 287MB/s]"
     }
    },
    "316439ab52b34990b53451b8b5e750e7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35b7a4545f60452bb0dd06e291a938ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "556e6e8219b24525a2d617f681a7eb4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7c42854744a41f3befb8d94e11e8569",
      "placeholder": "​",
      "style": "IPY_MODEL_03f08900a93f49978ed1c0eec1935b73",
      "value": "100%"
     }
    },
    "748a88c0d3dc4dc1804abbe8fb965f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d38f66105780496096a1938171b7684f",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffc86dccd84b495d99c2afe1ba29f2e6",
      "value": 102530333
     }
    },
    "7866c5c0bc594ddfb36bcd26b27abce1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8948573319f8422d9d224f6d682ea882": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "985a064893f54dde8768961c94385bab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b6ca0eb5b52a44d2ab1c5a3ef9b44e21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2b76932f6bc47d88849f9cd94d0be41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13c356d920864bd69ac525d804c88616",
       "IPY_MODEL_d591ed72b8f6485a837a281a02c46548",
       "IPY_MODEL_033f1884b5824b1c94a16ca824448ed7"
      ],
      "layout": "IPY_MODEL_0e5f8561330f4818bf48cbb302524b04"
     }
    },
    "ca9add9b81664a96a55f4cf3fd03c053": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d38f66105780496096a1938171b7684f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d591ed72b8f6485a837a281a02c46548": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_316439ab52b34990b53451b8b5e750e7",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca9add9b81664a96a55f4cf3fd03c053",
      "value": 100
     }
    },
    "e7c42854744a41f3befb8d94e11e8569": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eee0e3462f2e4ab7bd67138c76f70835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_556e6e8219b24525a2d617f681a7eb4e",
       "IPY_MODEL_748a88c0d3dc4dc1804abbe8fb965f09",
       "IPY_MODEL_1a8a4eb4d1bd40dfaa75e7132045bb08"
      ],
      "layout": "IPY_MODEL_f9873dff5d684e558ee8cb3e91fd6343"
     }
    },
    "f1c8cd1e18e74610a943b2e05ab96781": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9873dff5d684e558ee8cb3e91fd6343": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffc86dccd84b495d99c2afe1ba29f2e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
